<!doctype html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta property="og:type" content="website">
<meta property="og:title" content="Mountain & Morning">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Mountain & Morning">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Mountain & Morning">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title> Mountain & Morning </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Mountain & Morning</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
    
      <p class="site-subtitle"></p>
    
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/03/22/try/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="iridium">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.gif">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Mountain & Morning">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Mountain & Morning" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/03/22/try/" itemprop="url">
                  try
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-03-22T01:03:27+08:00">
                2017-03-22
              </time>
            

            

            
          </span>

          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>dsfsdaf</p>
<p>\begin{equation}<br>dad<br>\end{equation}</p>
<p>\begin{equation}\label{MM}<br>\min_G \max_D \left( \mathbb E_{\mathbf x \sim p_{\text{data}} (\mathbf x)} \left[ \log D(\mathbf x) \right] + \mathbb E_{\mathbf z \sim p_{\mathbf z} (\mathbf z)} \left[ 1 - \log D(G(\mathbf z)) \right] \right).<br>\end{equation}</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/02/02/GoDec/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="iridium">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.gif">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Mountain & Morning">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Mountain & Morning" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/02/02/GoDec/" itemprop="url">
                  GoDec
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-02-02T09:58:42+08:00">
                2017-02-02
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Study/" itemprop="url" rel="index">
                    <span itemprop="name">Study</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>This article is the notebook when I reading the article “GoDec: Randomized Low-rank &amp; Sparse Matrix Decomposition in Noisy Case”</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>What is GoDec?</li>
<li>How to perform GoDec?</li>
<li>How to accelerate GoDec?</li>
<li>The important variant of GoDec.</li>
<li>The converges proverty of GoDec.</li>
<li>What is the difference of GoDec</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li><p>“Go Decomposition” (GoDec) is a matrix docomposition algorithm that can efficiently and robustly estimate the low-rank part $L$ and the sparse part $S$ of a matrix $X = L + S + G$ with noise $G$, <em>i.e.</em>,<br>\begin{equation}<br>X = L + S + G, \text{rank}(L) \leq r, \text{card}(S) \leq k,<br>\end{equation}<br>where $G$ is the noise.</p>
</li>
<li><p>GoDec alternatively assigns the $r$-rank approximation of $X − S$ to $L$ and assigns the sparse approximation with cardinality $k$ of $X − L$ to $S$. The updating of $L$ is obtained via singular value hard thresholding of $X − S$, while the updating of $S$ is obtained via entry-wise hard thresholding  of $X − L$.</p>
</li>
<li>Bilateral random projections (BRP) besed low-rank approximation can accelearting the $r$-rank approximation of $X - S$ in GoDec.</li>
<li>An important variant of GoDec is matrix completion.</li>
<li>The objective value $\lVert X − L − S \rVert^2_F$ monotonically decreases and converges to a local minimum. Since the updating of $L$ and $S$ in GoDec is equivalent to alternatively projecting $L$ or 4S$ onto two smooth manifolds. The asymptotic and convergence speeds are mainly determined by the angle between the two manifolds.</li>
<li>GoDec and RPCA can expore the low-rank and sparse structures in $X$, but they are intrinsically different.</li>
</ul>
<h2 id="Bilateral-random-projections-BRP-based-low-rank-approximation"><a href="#Bilateral-random-projections-BRP-based-low-rank-approximation" class="headerlink" title="Bilateral random projections (BRP) based low-rank approximation"></a>Bilateral random projections (BRP) based low-rank approximation</h2><h3 id="Bilateral-random-projections"><a href="#Bilateral-random-projections" class="headerlink" title="Bilateral random projections"></a>Bilateral random projections</h3><p>Bilateral random projections is a fast low-rank approximation method, <em>i.e.</em>, $Y_1 = XA_1$ and $Y_2 = X^T A_2$, wherein $A_1 \in \mathbb R^{n \times r}$ and $A_2 \in \mathbb R^{m \times r}$ are random matrices,</p>
<p>\begin{equation}\label{BRP}<br>L = Y_1(A_2^T Y_1)^{-1} Y_2^T<br>\end{equation}</p>
<p>is a fast rank-$r$ approximation of $X$.</p>
<p>In order to improve the approximation precision of $L$ in \eqref{BRP}, we can use $Y_1, Y_2$ to build better $A_2, A_1$ respectively, <em>i.e.</em>,</p>
<p style="color:red;">Why?</p>

<p>\begin{equation}<br>A_1 \rightarrow Y_1 = XA_1 \rightarrow A_2 = Y_1 \rightarrow Y_2 = X^T A_2 \rightarrow A_1 = Y_2 \rightarrow Y_1 = XA_1<br>\end{equation}</p>
<h3 id="Power-scheme-modification"><a href="#Power-scheme-modification" class="headerlink" title="Power scheme modification"></a><a href="https://papers.nips.cc/paper/1398-em-algorithms-for-pca-and-spca.pdf" target="_blank" rel="external">Power scheme</a> modification</h3><p><strong>First</strong>, Calculate BRP of matrix $\tilde{X} = (XX^T)^q X$:</p>
<p>\begin{equation}<br>\tilde{L} = Y_1 (A_2^T Y_1)^{-1} Y_2^T.<br>\end{equation}</p>
<p><strong>Second</strong>, calculte the QR decomposition of $Y_1$ and $Y_2$, <em>i.e.</em>,</p>
<p>\begin{equation}<br>Y_1 = Q_1 R_1, Y_2 = Q_2 R_2.<br>\end{equation}</p>
<p><strong>Third</strong>, the low-rank approximation of $X$ is then given by:</p>
<p>\begin{equation}<br>L = (\tilde{L})^{\frac{1}{2q + 1}} = Q_1 \left[R_1 (A_2^T Y_1)^{-1} R_2^T\right]^{\frac{1}{2q + 1}} Q^T_2.<br>\end{equation}</p>
<h2 id="Go-Decomposition"><a href="#Go-Decomposition" class="headerlink" title="Go Decomposition"></a>Go Decomposition</h2>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/02/01/Random-projection/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="iridium">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.gif">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Mountain & Morning">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Mountain & Morning" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/02/01/Random-projection/" itemprop="url">
                  Random projection
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-02-01T09:41:21+08:00">
                2017-02-01
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Study/" itemprop="url" rel="index">
                    <span itemprop="name">Study</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>References of this article:</p>
<ul>
<li><a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=5&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjEsN7t3O3RAhXFWbwKHfzJCnUQFgg7MAQ&amp;url=http%3A%2F%2Fwww.cims.nyu.edu%2F~cfgranda%2Fpages%2FOBDA_spring16%2Fmaterial%2Frandom_projections.pdf&amp;usg=AFQjCNE1J6X1mj4D3mfYk89w6RPzmq-Teg&amp;sig2=jT_V_1KQrZ9XYLrPztmgjg&amp;bvm=bv.146073913,d.dGc" target="_blank" rel="external">Random projections</a></li>
<li><a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjEsN7t3O3RAhXFWbwKHfzJCnUQFggpMAI&amp;url=https%3A%2F%2Fwww.ma.utexas.edu%2Fusers%2Frward%2Fmadison14.pdf&amp;usg=AFQjCNHyL4EqJLUMh9riOFEr5V7-3sjNZQ&amp;sig2=wEim9AvmVKy1fzsy518wqQ&amp;bvm=bv.146073913,d.dGc" target="_blank" rel="external">Dimension reduction via random projections</a></li>
</ul>
<p>Random projections are a useful tool in the analysis and processing of high-dimensional data, which can used in dimensionality reduction.</p>
<h2 id="Dimendionality-reduction"><a href="#Dimendionality-reduction" class="headerlink" title="Dimendionality reduction"></a>Dimendionality reduction</h2><p>The goal of dimensionality-reduction techniques is to project high dimensional data onto a lower dimensional space while preserving as much information as possible. We denote the dimension of the high-dimensional space by $n$ and the dimension of the low-dimensional subspace by $m$. We will focus on dimensionality reduction via linear projections.</p>
<p><strong>Definition</strong> (Linear projection). The linear projection of $x \in \mathbb{R}^n$ onto a subspace $\mathcal S \subseteq \mathbb R^n$<br>is the point of $\mathcal S$ that is closest to $x$, <em>i.e.</em> the solution to the optimization problem</p>
<p>\begin{align}<br>\text{minimiize} &amp; \qquad \Vert x - u \Vert \\\<br>\text{subject to} &amp; \qquad u \in \mathcal S.<br>\end{align}</p>
<p>The following simple lemma explains how to compute a projection using an orthonormal basis of the subspace that we want to project onto。</p>
<p><strong>Lemma</strong> Let $U$ be a matrix whose columns are an orthonormal basis of a subspace $\mathcal S \subseteq \mathbb R^n$.</p>
<p>\begin{equation}<br>\mathcal P_\mathcal S(x) = UU^T x.<br>\end{equation}</p>
<p>Once we fix a projection and a corresponding matrix $U$ the lower-dimensional representation of a vector $x \in \mathbb R^n$ is $U^T x \in \mathbb R^m$, so the dimensionality is reduced from $n$ to $m$. An interesting problem is how to choose the low-dimensional subspace parametrized by $U$.</p>
<p>The popular choices are principal component analysis and random projections.</p>
<h2 id="Random-projections"><a href="#Random-projections" class="headerlink" title="Random projections"></a>Random projections</h2><p>To apply PCA we need to process all of the data points beforehand in order to compute the projection. This may be too computationally costly if the dataset is very large or not possible at all if the aim is to project a stream of data in real time. For such cases we need a non-adaptive alternative to PCA that chooses the projection before actually seeing the data.</p>
<p>A simple method that tends to work well is to project onto a random subspace. In particular, if the data points are $x_1, x_2, \dotsc \in \mathbb R^n$, we can obtain a random projection by multiplying the data with a random matrix $A \in \mathbb R^{m×n}$ to obtain $Ax_1, Ax_2, \dotsc \in \mathbb R^m$. Strictly speaking, this is a linear projection only if $A$ is a projection matrix with orthonormal rows. However, in many cases the rows of random matrices are approximately orthogonal and consequently this procedure yields an approximate projection.</p>
<p>Dimensionality-reduction techniques are useful if they preserve the information that we are interested in. In many cases, we would like the projection to conserve the distances between the different data points. This allows to apply algorithms such as nearest neighbors in the lower-dimensional space. The following lemma guarantees that random projections do not distort the distances between points with a certain probability.</p>
<p>Here we talk about the result for a matrix with Gaussian entries, it can also be extended to other matrices that can be applied more efficiently.</p>
<p><strong>Lemma</strong> (Johnson-Lindenstrauss lemma). Let $\mathcal S := \{x_1, \dotsc, x_k\}$ in $\mathbb R^n$. There exists a random function $f$ such that for any pair of points $x_i, x_j$</p>
<p>\begin{equation}<br>(1 - \epsilon) \lVert x_i - x_j \rVert_2^2 \leq \lVert f(x_i) - f(x_j) \rVert_2^2 \leq (1 + \epsilon) \lVert x_i - x_j \rVert_2^2,<br>\end{equation}</p>
<p>with probability at least $\frac{1}{k}$ as long as </p>
<p>\begin{equation}<br>m \geq \frac{8 \log(k)}{\epsilon^2}.<br>\end{equation}</p>
<p>The random function is of the form</p>
<p>\begin{equation}<br>f(x) := \frac{1}{m} Ax<br>\end{equation}</p>
<p>and $A$ is a matrix with <em>i.i.d.</em> Caussian entries with zero mean and unit variance.</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/01/09/读过的令人感动的资料/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="iridium">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.gif">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Mountain & Morning">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Mountain & Morning" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/01/09/读过的令人感动的资料/" itemprop="url">
                  读过的令人感动的资料
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-01-09T23:09:48+08:00">
                2017-01-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/令人感动的资料/" itemprop="url" rel="index">
                    <span itemprop="name">令人感动的资料</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>很多人都觉得数学很难，特别是随着知识的增长，我有时也会有这样的感觉。但慢慢发现，问题的关键其实不是数学难，而是教材太烂，数学最根本的是其中的逻辑，不同的数学只是建立在不同的定义和公理上罢了。所以有时候在漫漫文海中发现一份对味的资料，感觉就像在北方的秋天一样。这一系列的文章主要就简单介绍了解相关领域时 Mountain 找到的好东西。既是分享，也算是自己的读书笔记。当然这些主要是 Mountain 读起来比较舒爽，不一定适合其他观众，权当个人建议。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/01/05/自编码堆叠/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="iridium">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.gif">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Mountain & Morning">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Mountain & Morning" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/01/05/自编码堆叠/" itemprop="url">
                  自编码堆叠
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-01-05T09:28:24+08:00">
                2017-01-05
              </time>
            

            

            
          </span>

          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文主要参考了 <a href="http://ufldl.stanford.edu/wiki/index.php/Stacked_Autoencoders" target="_blank" rel="external">Stacked Autoencoders</a>.</p>
<p>自编码堆叠可以看成是由多个自编码组成的神经网络。一个自编码网络有三层，输入层、隐层、输出层。隐层的值可以看做是对输入层数据内部结构的提取。自编码堆叠的基本思想就是依次训练稀疏自编码，将上一个自编码的隐层数据作为下一个自编码的输入。从而不断提取数据更深层次的信息。</p>
<p>自编码的一个重要应用是用来对神经网络预训练，下面就是一个这样的例子。</p>
<p>首先，用原始数据作为输入，训练一个稀疏自编码网络</p>
<img src="/2017/01/05/自编码堆叠/Stacked_SparseAE_Features1.png" alt="Stacked_SparseAE_Features1.png" title="">
<p>训练完成后，再用原始数据在隐层的输出值作为输入训练一个新的自编码网络</p>
<img src="/2017/01/05/自编码堆叠/Stacked_SparseAE_Features2.png" alt="Stacked_SparseAE_Features2.png" title="">
<p>最后再用第二个稀疏自编码的隐层数据训练一个 softmax 分类器</p>
<img src="/2017/01/05/自编码堆叠/Stacked_Softmax_Classifier.png" alt="Stacked_Softmax_Classifier.png" title="">
<p>将第一个稀疏自编码的输入和 softmax 分类器的输出以及每个稀疏自编码的隐层结合起来就可以构成一个完整的分类网络</p>
<p><img src="/2017/01/05/自编码堆叠/Stacked_Combined.png" alt="$}</p> <p>预训练的网络参数可以作为初始值，在此基础上可以继续训练整个分类器网络。</p>" title="$}</p> <p>预训练的网络参数可以作为初始值，在此基础上可以继续训练整个分类器网络。</p>"></p>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/01/05/稀疏自编码/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="iridium">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.gif">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Mountain & Morning">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Mountain & Morning" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/01/05/稀疏自编码/" itemprop="url">
                  稀疏自编码
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-01-05T09:25:42+08:00">
                2017-01-05
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Study/" itemprop="url" rel="index">
                    <span itemprop="name">Study</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文主要参考了 <a href="http://ufldl.stanford.edu/wiki/index.php/Autoencoders_and_Sparsity" target="_blank" rel="external">Autoencoders and Sparsity</a></p>
<p>自编码神经网络是一种非监督学习方法，将无标签数据集的标签等于其输入， $\mathbf y^{(k)} = y^{(k)}$, 就可以将其转化为一般的神经网络。如此，无标签的训练集 $\{\mathbf x^{(1)}, \mathbf x^{(2)}, \dotsc, \mathbf x^{(m)} \}$ 就变成了标签等于输入的有标签训练集 $\{(\mathbf x^{(1)}, \mathbf y^{(1)}), \dotsc, (\mathbf x^{(m)}, \mathbf y^{(m)})\}$.</p>
<p>如图为一个只有一个隐层的自编码网络</p>
<img src="/2017/01/05/稀疏自编码/Autoencoder636.png" alt="Autoencoder636.png" title="">
<p>自编码试图学习一个函数 $h_{\mathcal{W, b}} (\mathbf x) \approx \mathbf x$. 也即一个近似的等值函数。表面上看这是一个平凡问题，但当给网络设置约束以后，就可能学习到数据的结构。比如可以限制隐层的节点数目 $s_2$. 也可以加稀疏约束来限制节点的输出值。</p>
<h2 id="稀疏约束"><a href="#稀疏约束" class="headerlink" title="稀疏约束"></a>稀疏约束</h2><p>记 $a_j^{(2)}$ 为隐层中第 $j$ 个元素的值，是前一层的输出后一层的输入。作为前一层的输出时可以记为 $a_j^{(2)} (\mathbf x)$. 每个训练样本在此节点的平均输出值记为 $\hat\rho_j$, 有</p>
<p>\begin{equation*}<br>\hat\rho_j = \frac{1}{m} \sum_{k = 1}^m \left[ a_j^{(a)} (\mathbf x^{(k)}) \right]<br>\end{equation*}</p>
<p>稀疏约束就是使 $\hat\rho_j$ 近似一个预先设定的稀疏参数 $\rho$. 即</p>
<p>\begin{equation*}<br>\hat\rho_j \approx \rho<br>\end{equation*}</p>
<p>当 $\rho$ 比较小的时候就要求大部分的训练样本在该节点的输出 $a_j^{(2)} (\mathbf x)$ 接近 $0$.</p>
<p>稀疏约束可以通过 KL 散度实现，其定义为</p>
<p>\begin{equation*}<br>\text{KL}(\rho \Vert \hat\rho_j) = \rho \log \frac{\rho}{\hat\rho_j} + (1 - \rho) \log \frac{1 - \rho}{1 - \hat\rho_j}.<br>\end{equation*}</p>
<p>KL 散度可以度量两个分布的差异。</p>
<p>如此，稀疏自编码网络的 cost function 就可以写成</p>
<p>\begin{equation*}<br>J_{\text sparse} (\mathcal{W, b}) = J(\mathcal{W, b}) + \beta \sum_{j = 1}^{s_2} \text{KL}(\rho \Vert \hat\rho_j)<br>\end{equation*}</p>
<p>问题又转化成了一个优化问题了。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/01/04/梯度下降算法/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="iridium">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.gif">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Mountain & Morning">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Mountain & Morning" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/01/04/梯度下降算法/" itemprop="url">
                  梯度下降算法
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-01-04T13:15:29+08:00">
                2017-01-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Study/" itemprop="url" rel="index">
                    <span itemprop="name">Study</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文主要参考了 <a href="http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm" target="_blank" rel="external">Backpropagation Algorithm</a></p>
<p>假设我们有一个<a href="https://thatfreesky.github.io/2017/01/03/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="external">神经网络</a>和 $m$ 个数据点的训练集 $\{(\mathbf x^{(1)}, \mathbf y^{(1)}), \dotsc, (\mathbf x^{(m)}, \mathbf y^{(m)})\}$. 对一个数据点 $(\mathbf x, \mathbf y)$ 可以定义 cost function 为</p>
<p>\begin{equation}\label{cf}<br>J(\mathcal W, \mathcal b; \mathbf x, \mathbf y) = \frac{1}{2} \lVert h_{\mathcal W, \mathcal b}(\mathbf x) - \mathbf y \rVert^2.<br>\end{equation}</p>
<p>对于所有数据点的全局 cost function 可以定义为</p>
<p>\begin{equation}\label{ocf}<br>J(\mathcal W, \mathcal b) = \left[ \frac{1}{m} \sum_{k = 1}^m J(\mathcal W, \mathcal b; \mathbf x^{(k)}, \mathbf y^{(k)}) \right] + \frac{\lambda}{2} \lVert \mathcal W \rVert<br>\end{equation}</p>
<p>其中，第一项为所有数据点的 cost function 函数平均值， 第二项是正则化项，可以减小权重值避免过拟合。</p>
<p>由此，训练网络的过程就是寻找合适的 $\mathcal W$ 和 $\mathcal b$ 使 cost function 最小化。首先，初始化参数 $(\mathcal W, \mathcal b)$, 再选择合适的优化算法如 batch gradient descent 求解。</p>
<p>梯度下降算法中，对于 $l$ 层的一个权值参数和一个偏置参数其更新方法应该是</p>
<p>\begin{equation*}<br>\begin{split}<br>W_{ij}^{(l)} &amp; = W_{ij}^{(l)} - \alpha \frac{\partial}{\partial W_{ij}^{(l)}} J(\mathcal W, \mathcal b)\\<br>b_i^{(l)} &amp; = b_i^{(l)} - \alpha \frac{\partial}{\partial b_i^{(l)}} J(\mathcal W, \mathcal b)<br>\end{split}<br>\end{equation*}</p>
<p>其中， $\alpha$ 是学习率。可知上式的关键是对模型的参数求偏微分，由 \eqref{ocf} 可知</p>
<p>\begin{align*}<br>\frac{\partial}{\partial W_{ij}^{(l)}} J(\mathcal W, \mathcal b) &amp; = \left[ \frac{1}{m} \sum_{k = 1}^m \frac{\partial}{\partial W_{ij}^{(l)}} J(\mathcal W, \mathcal b; \mathbf x^{(k)}, \mathbf y^{(k)}) \right] + \lambda W_{ij}^{(l)}\\<br>\frac{\partial}{\partial b_i^{(l)}} J(\mathcal W, \mathcal b) &amp; = \frac{1}{m} \sum_{k = 1}^m \frac{\partial}{\partial b_{i}^{(l)}} J(\mathcal W, \mathcal b; \mathbf x^{(k)}, \mathbf y^{(k)})<br>\end{align*}</p>
<p>反向传播算法提供了一种有效计算 $\frac{\partial}{\partial W_{ij}^{(l)}} J(\mathcal W, \mathcal b; \mathbf x^{(k)}, \mathbf y^{(k)})$ 和 $\frac{\partial}{\partial b_{i}^{(l)}} J(\mathcal W, \mathcal b; \mathbf x^{(k)}, \mathbf y^{(k)})$ 的方法。</p>
<p>基于此，我们可以重写矩阵形式的梯度下降算法</p>
<ul>
<li>对网络的每一层，设 $\Delta \mathbf W^{(l)}$ 为与 $\mathbf W^{(l)}$ 形状相同的 $\mathbf 0$ 矩阵， $\Delta \mathbf b^{(l)}$ 为与 $\mathbf b^{(l)}$ 形状相同的 $\mathbf 0$ 向量。</li>
<li>对 $k = 1$ 到 $m$,<ul>
<li>用反向传播算法计算 $\nabla_{\mathbf W^{(l)}} J(\mathcal W, \mathcal b; \mathbf x^{(k)}, \mathbf y^{(k)})$ 和 $\nabla_{\mathbf b^{(l)}} J(\mathcal W, \mathcal b; \mathbf x^{(k)}, \mathbf y^{(k)})$.</li>
<li>$\Delta \mathbf W^{(l)} = \Delta \mathbf W^{(l)} \oplus \nabla_{\mathbf W^{(l)}} J(\mathcal W, \mathcal b; \mathbf x^{(k)}, \mathbf y^{(k)})$.</li>
<li>$\Delta \mathbf b^{(l)} = \Delta \mathbf b^{(l)} \oplus \nabla_{\mathbf b^{(l)}} J(\mathcal W, \mathcal b; \mathbf x^{(k)}, \mathbf y^{(k)})$.</li>
</ul>
</li>
<li>更新每一层的参数<br>\begin{align*}<br>\mathbf W^{(l)} &amp; = \mathbf W^{(l)} - \alpha \left[\left( \frac{1}{m} \Delta \mathbf W^{(l)} \right) + \mathbf\lambda \mathbf W^{(l)} \right]\\<br>\mathbf b^{(l)} &amp; = \mathbf b^{(l)} - \alpha \left[ \frac{1}{m} \Delta \mathbf b^{(l)} \right]<br>\end{align*}</li>
</ul>
<p>其中 $\oplus$ 表示矩阵元素对应相加。</p>
<p>总的来说，对于一个已有的神经网络模型，网络的结构、参数的形式都已经是确定的了，是不随训练样本改变的。所谓的训练，就是使模型的参数适应训练样本。这时，就将求解模型转换为了一个优化问题。如果我们有 $m$ 个训练样本，那么我们训练模型的终极目标是最小化 \eqref{ocf}. 梯度下降算法是每次迭代时，利用所有点来更新系数，使系数适应所有点，也就是其目标函数就是 \eqref{ocf}. 当 $m$ 很大时，每次迭代的计算成本会很高。随机梯度下降算法则是每次随机选取一个数据点来更新系数，使系数适应这个点，其目标函数是 \eqref{cf}. 其想法就是使参数不断适应随机选择的点从而期望参数最终能适应所有的点。考虑到 \eqref{cf} 和 \eqref{ocf} 的关系，这种思想也是不无道理的。Batch gradient descent 算法则是介于二者之间的。</p>
<p>梯度下降算法是一种优化算法，而反向传播算法是一种求解偏微分的计算方法。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/01/03/神经网络/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="iridium">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.gif">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Mountain & Morning">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Mountain & Morning" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/01/03/神经网络/" itemprop="url">
                  神经网络
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-01-03T23:14:15+08:00">
                2017-01-03
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Study/" itemprop="url" rel="index">
                    <span itemprop="name">Study</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文主要参考了 <a href="http://ufldl.stanford.edu/wiki/index.php/Neural_Networks" target="_blank" rel="external">Neural Networks</a><br>神经网络是一种非线性分类器。</p>
<h2 id="神经元"><a href="#神经元" class="headerlink" title="神经元"></a>神经元</h2><p>神经元是神经网络的基本单位，类似于<a href="https://zh.wikipedia.org/wiki/%E6%84%9F%E7%9F%A5%E5%99%A8" target="_blank" rel="external">感知机</a>. 若输入用向量 $\mathbf x = (x_1, x_2, x_3, 1)$ 表示输入，$h_{\mathbf W, b}(\mathbf x)$, 表示输出，$\mathbf W, b$ 为参数。为了表示方便加入了一个偏置项 “+1”，来结合线性函数中的截断项 $b$. 一个神经元就可以看做是一个输入空间到输出空间的映射，具体来讲是一个线性函数和一个非线性激励函数的复合映射：</p>
<p>$$<br>h_{\mathbf W, b}(\mathbf x) = f(\mathbf W^T \mathbf x) = f(\sum_{i = 1}^3 W_i x_i +b)<br>$$</p>
<p>其中 $f(\mathcal z) : \mathcal R \rightarrow \mathcal R$ 为<a href="https://en.wikipedia.org/wiki/Activation_function" target="_blank" rel="external">激励函数</a>，常用的如 sigmoid 函数：</p>
<p>$$<br>f(\mathcal z) = \frac{1}{1 + \exp(- z)}<br>$$</p>
<p>如图<br><img src="/2017/01/03/神经网络/SingleNeuron.png" alt="Single Neuron" title="Single Neuron"></p>
<h2 id="神经网络模型"><a href="#神经网络模型" class="headerlink" title="神经网络模型"></a>神经网络模型</h2><p>如上所说，神经元本质是一个从输入空间到输出空间的映射，如同复合映射的存在，神经元也可以组合在一起形成神经网络。事实上任意的复合映射都可以用一个神经网络来表示，区别就在于网络结构和每个节点上的映射。下图是一个简单的三层神经网络模型。</p>
<img src="/2017/01/03/神经网络/Network331.png" alt="Network331.png" title="">
<p>$L_1$ 为输入层，$L_2$ 为隐层，$L_3$ 为输出层，构成了一个331的网络。对于一个有 $n_L$ 层的神经网络，第 $l$ 层的输入向量为 $\mathbf a^{(l)}$, 参数为系数矩阵 $\mathbf W^{(l)}$ 和截距向量 $\mathbf b^{(l)}$. 则神经网络的参数为 $(\mathcal W, \mathcal b) = (\mathbf W^{(1)}, \mathbf b^{(1)}, \dotsc, \mathbf W^{(n_L)}, \mathbf b^{(n_L)})$. 如果我们以 $f([z_1, z_2]) = [f(z_1), f(z_2)]$ 的形式扩展激励函数，使之可以接受向量为输入值，那么模型可以表示为</p>
<p>\begin{align*}<br>\mathbf z^{(l+1)} &amp; = \mathbf W^{(l)} \mathbf a^{(l)} + \mathbf b^{(l)}\\<br>\mathbf a^{(l+1)} &amp; = f(\mathbf z^{(l + 1)})<br>\end{align*}</p>
<p>可以看出，除了第一层以外，其它各层的输入空间都是激励函数的值域空间。每一层的输出即为下一层的输入。</p>
<h2 id="多任务神经网络"><a href="#多任务神经网络" class="headerlink" title="多任务神经网络"></a>多任务神经网络</h2><p>如果神经网络不止有一个输出，就构成一个多任务网络，如下图</p>
<img src="/2017/01/03/神经网络/Network3322.png" alt="Network3322.png" title="">
<p>训练这种模型也需要相应的多维标签的样本，即 $(x^{(i)}, y^{(i)})$, 其中 $y^{(i)} \in \mathcal R^2$. 这种网络可以处理多分类问题。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="iridium" />
          <p class="site-author-name" itemprop="name">iridium</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">8</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-categories">
              
                <span class="site-state-item-count">2</span>
                <span class="site-state-item-name">分类</span>
              
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">2</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">iridium</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  



  




	





  




  
  

  

  

  

  


</body>
</html>
